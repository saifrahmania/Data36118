{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "130d1642501043e285dd5a847dabb928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Text:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b552591d56f9443f9daa1e523ef96e4a",
            "placeholder": "Enter your text here",
            "rows": null,
            "style": "IPY_MODEL_013d37c82aab40e4b7d2ef349d438b15",
            "value": ""
          }
        },
        "b552591d56f9443f9daa1e523ef96e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "013d37c82aab40e4b7d2ef349d438b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b92015563e2a4ba89f901af43dfd10c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_09793d9cbb6f458d9bf90fc4cf48846d",
            "style": "IPY_MODEL_07fa2164d068445eb11e79b6ed51e473",
            "tooltip": ""
          }
        },
        "09793d9cbb6f458d9bf90fc4cf48846d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07fa2164d068445eb11e79b6ed51e473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basics of Natural Language Processing (NLP)Take Home Exercise #\n",
        "\n"
      ],
      "metadata": {
        "id": "8AhFOUj6iCWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following link to find open source data sets to complete take-home exercises.\n",
        "\n",
        "[Data Sets](https://opendatascience.com/20-open-datasets-for-natural-language-processing/)\n",
        "\n",
        "Or, you can try out Assignment 1 data set for a head start to the work!"
      ],
      "metadata": {
        "id": "zwEukFd8AJE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this code in the beginning to limit the output size of the cells"
      ],
      "metadata": {
        "id": "fxBFGZ8DadFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from IPython.display import display, Javascript\n",
        "\n",
        "def resize_colab_cell():\n",
        "  # Change the maxHeight variable to change the max height of the output\n",
        "   display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 400})'))\n",
        "  #Change output size for the entire notebook (set to call function on cell run)\n",
        "   get_ipython().events.register('pre_run_cell', resize_colab_cell)"
      ],
      "metadata": {
        "id": "8HBE7ck1XPX_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Input Text\n",
        "\n",
        "Write a function to collect text data for the analysis via user input - E.g. from a text box\n",
        "\n"
      ],
      "metadata": {
        "id": "jT3X0RuyndZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "def collect_text_data():\n",
        "    text_box = widgets.Textarea(\n",
        "        placeholder='Enter your text here',\n",
        "        description='Text:',\n",
        "        disabled=False\n",
        "    )\n",
        "    display(text_box)\n",
        "\n",
        "    button = widgets.Button(description=\"Submit\")\n",
        "    display(button)\n",
        "\n",
        "    def handle_submit(sender):\n",
        "        user_input = text_box.value.strip()\n",
        "        print(\"Collected Text Data:\", user_input)\n",
        "\n",
        "    button.on_click(handle_submit) # Use button.on_click instead\n",
        "\n",
        "collect_text_data()"
      ],
      "metadata": {
        "id": "ER80G_dwRkij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95,
          "referenced_widgets": [
            "130d1642501043e285dd5a847dabb928",
            "b552591d56f9443f9daa1e523ef96e4a",
            "013d37c82aab40e4b7d2ef349d438b15",
            "b92015563e2a4ba89f901af43dfd10c7",
            "09793d9cbb6f458d9bf90fc4cf48846d",
            "07fa2164d068445eb11e79b6ed51e473"
          ]
        },
        "outputId": "6c281cc1-8712-40e0-c104-a126c0d7db8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='Text:', placeholder='Enter your text here')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "130d1642501043e285dd5a847dabb928"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Submit', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b92015563e2a4ba89f901af43dfd10c7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Basic Analysis\n",
        "\n",
        "Perform basic text analysis on the collected text using Spacy ([spacy.io](http://spacy.io)) library."
      ],
      "metadata": {
        "id": "UI_3GXkzn5U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def analyze_text(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    print(\"Tokens:\")\n",
        "    for token in doc:\n",
        "        print(token.text, token.pos_, token.dep_)\n",
        "\n",
        "    print(\"\\nSentences:\")\n",
        "    for sent in doc.sents:\n",
        "        print(sent.text)\n",
        "\n",
        "    print(\"\\nNamed Entities:\")\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.label_)\n",
        "\n",
        "\n",
        "user_input = '''\n",
        "36118 Applied Natural Language Processing\n",
        "\n",
        "Subject description\n",
        "This subject introduces you to the complexities of analysing human language data and the use of Natural Language Processing (NLP) and text mining techniques. You'll develop both technical and communicative skills to process and interpret unstructured textual data. Covering core NLP concepts and extraction techniques, the course equips you with … For more content click the Read More button below.\n",
        "\n",
        "Learning outcomes\n",
        "Upon completion of this subject, graduates will be able to:\n",
        "1. Understand core concepts of Natural Language Processing (NLP) and computational linguistics including its limitations (CILO 2.2, 2.3)\n",
        "\n",
        "2.Evaluate complex challenges for problem solving and build practical NLP applications (CILO 2.3, 4.2)\n",
        "\n",
        "3. Apply text mining techniques on unstructured data sets using advanced NLP programming packages (CILOs 1.2, 2.2)\n",
        "\n",
        "4. Interpret, extract value and effectively communicate insights from text analysis and create real-world applications suitable to a range of audiences (CILOs 2.4, 3.2, 4.2)\n",
        "\n",
        "5. Articulate the strengths, weaknesses and underlying assumptions of NLP and text analysis to apply ethical practices (CILO 5.1, 5.2)\n",
        "\n",
        "Learning and teaching activities\n",
        "Blend of online and face to face activities: The subject is offered through a series of teaching sessions which blend online and face-to-face learning. Students learn through interactive lectures and classroom activities making use of the subject materials on canvas. They also engage in individual and collaborative learning activities to … For more content click the Read More button below. Authentic problem based learning: This subject offers a range of authentic data science problems to solve that will help develop students’ text analysis skills. They work on real world data analysis problems for broad areas of interest using unstructured data and contemporary techniques. Collaborative work: Group activities will enable students to leverage peer-learning and demonstrate effective team participation, as well as learning to work in professional teams with an appreciation of diverse perspectives on data science and innovation. Future-oriented strategies: Students will be exposed to contemporary learning models using speculative thinking, ethical and human-centered approaches as well as reflection. Electronic portfolios will be used to curate, consolidate and provide evidence of learning and development of course outcomes, graduate attributes and professional evolution. Formative feedback will be offered with all assessment activities for successful engagement.\n",
        "\n",
        "Authentic problem based learning: This subject offers a range of authentic data science problems to solve that will help develop students’ text analysis skills. They work on real world data analysis problems for broad areas of interest using unstructured data and contemporary techniques.\n",
        "\n",
        "Collaborative work: Group activities will enable students to leverage peer-learning and demonstrate effective team participation, as well as learning to work in professional teams with an appreciation of diverse perspectives on data science and innovation.\n",
        "\n",
        "Future-oriented strategies: Students will be exposed to contemporary learning models using speculative thinking, ethical and human-centered approaches as well as reflection. Electronic portfolios will be used to curate, consolidate and provide evidence of learning and development of course outcomes, graduate attributes and professional evolution. Formative feedback will be offered with all assessment activities for successful engagement.\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "analyze_text(user_input)\n"
      ],
      "metadata": {
        "id": "n8WSsfmTj8en",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f5436b-310a-4cc7-f502-b7a9925f4bb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "\n",
            " SPACE dep\n",
            "36118 NUM nummod\n",
            "Applied PROPN compound\n",
            "Natural PROPN compound\n",
            "Language PROPN compound\n",
            "Processing PROPN compound\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Subject PROPN compound\n",
            "description NOUN nsubj\n",
            "\n",
            " SPACE dep\n",
            "This DET det\n",
            "subject NOUN nsubj\n",
            "introduces VERB ROOT\n",
            "you PRON dobj\n",
            "to ADP prep\n",
            "the DET det\n",
            "complexities NOUN pobj\n",
            "of ADP prep\n",
            "analysing VERB pcomp\n",
            "human ADJ amod\n",
            "language NOUN compound\n",
            "data NOUN dobj\n",
            "and CCONJ cc\n",
            "the DET det\n",
            "use NOUN conj\n",
            "of ADP prep\n",
            "Natural PROPN compound\n",
            "Language PROPN nmod\n",
            "Processing PROPN pobj\n",
            "( PUNCT punct\n",
            "NLP PROPN appos\n",
            ") PUNCT punct\n",
            "and CCONJ cc\n",
            "text NOUN conj\n",
            "mining NOUN compound\n",
            "techniques NOUN dobj\n",
            ". PUNCT punct\n",
            "You PRON nsubj\n",
            "'ll AUX aux\n",
            "develop VERB ROOT\n",
            "both CCONJ det\n",
            "technical ADJ amod\n",
            "and CCONJ cc\n",
            "communicative ADJ conj\n",
            "skills NOUN dobj\n",
            "to PART aux\n",
            "process VERB relcl\n",
            "and CCONJ cc\n",
            "interpret VERB conj\n",
            "unstructured ADJ amod\n",
            "textual ADJ amod\n",
            "data NOUN dobj\n",
            ". PUNCT punct\n",
            "Covering VERB advcl\n",
            "core NOUN compound\n",
            "NLP PROPN compound\n",
            "concepts NOUN dobj\n",
            "and CCONJ cc\n",
            "extraction NOUN compound\n",
            "techniques NOUN conj\n",
            ", PUNCT punct\n",
            "the DET det\n",
            "course NOUN nsubj\n",
            "equips VERB ROOT\n",
            "you PRON dobj\n",
            "with ADP prep\n",
            "… PUNCT punct\n",
            "For ADP prep\n",
            "more ADJ amod\n",
            "content NOUN pobj\n",
            "click VERB conj\n",
            "the DET det\n",
            "Read PROPN npadvmod\n",
            "More ADJ amod\n",
            "button NOUN dobj\n",
            "below ADV advmod\n",
            ". PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Learning VERB compound\n",
            "outcomes NOUN nsubj\n",
            "\n",
            " SPACE dep\n",
            "Upon SCONJ prep\n",
            "completion NOUN pobj\n",
            "of ADP prep\n",
            "this DET det\n",
            "subject NOUN pobj\n",
            ", PUNCT punct\n",
            "graduates NOUN nsubj\n",
            "will AUX aux\n",
            "be AUX ROOT\n",
            "able ADJ acomp\n",
            "to PART xcomp\n",
            ": PUNCT punct\n",
            "\n",
            " SPACE dep\n",
            "1 X meta\n",
            ". PUNCT punct\n",
            "Understand VERB ROOT\n",
            "core NOUN compound\n",
            "concepts NOUN dobj\n",
            "of ADP prep\n",
            "Natural PROPN compound\n",
            "Language PROPN compound\n",
            "Processing PROPN pobj\n",
            "( PUNCT punct\n",
            "NLP PROPN appos\n",
            ") PUNCT punct\n",
            "and CCONJ cc\n",
            "computational ADJ amod\n",
            "linguistics NOUN conj\n",
            "including VERB prep\n",
            "its PRON poss\n",
            "limitations NOUN pobj\n",
            "( PUNCT punct\n",
            "CILO PROPN appos\n",
            "2.2 NUM nummod\n",
            ", PUNCT punct\n",
            "2.3 NUM conj\n",
            ") PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "2.Evaluate NOUN nummod\n",
            "complex ADJ amod\n",
            "challenges NOUN appos\n",
            "for ADP prep\n",
            "problem NOUN pobj\n",
            "solving VERB acl\n",
            "and CCONJ cc\n",
            "build VERB conj\n",
            "practical ADJ amod\n",
            "NLP PROPN compound\n",
            "applications NOUN dobj\n",
            "( PUNCT punct\n",
            "CILO PROPN appos\n",
            "2.3 NUM nummod\n",
            ", PUNCT punct\n",
            "4.2 NUM nummod\n",
            ") PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "3 NUM nummod\n",
            ". PUNCT punct\n",
            "Apply VERB ROOT\n",
            "text NOUN compound\n",
            "mining NOUN compound\n",
            "techniques NOUN dobj\n",
            "on ADP prep\n",
            "unstructured ADJ amod\n",
            "data NOUN compound\n",
            "sets NOUN pobj\n",
            "using VERB acl\n",
            "advanced ADJ amod\n",
            "NLP PROPN compound\n",
            "programming NOUN compound\n",
            "packages NOUN dobj\n",
            "( PUNCT punct\n",
            "CILOs PROPN appos\n",
            "1.2 NUM nummod\n",
            ", PUNCT punct\n",
            "2.2 NUM nummod\n",
            ") PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "4 X punct\n",
            ". PUNCT punct\n",
            "Interpret VERB nsubj\n",
            ", PUNCT punct\n",
            "extract VERB ROOT\n",
            "value NOUN dobj\n",
            "and CCONJ cc\n",
            "effectively ADV advmod\n",
            "communicate VERB conj\n",
            "insights NOUN dobj\n",
            "from ADP prep\n",
            "text NOUN compound\n",
            "analysis NOUN pobj\n",
            "and CCONJ cc\n",
            "create VERB conj\n",
            "real ADJ amod\n",
            "- PUNCT punct\n",
            "world NOUN compound\n",
            "applications NOUN dobj\n",
            "suitable ADJ amod\n",
            "to ADP prep\n",
            "a DET det\n",
            "range NOUN pobj\n",
            "of ADP prep\n",
            "audiences NOUN pobj\n",
            "( PUNCT punct\n",
            "CILOs PROPN appos\n",
            "2.4 NUM nummod\n",
            ", PUNCT punct\n",
            "3.2 NUM nummod\n",
            ", PUNCT punct\n",
            "4.2 NUM nummod\n",
            ") PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "5 NUM nummod\n",
            ". PUNCT punct\n",
            "Articulate VERB ccomp\n",
            "the DET det\n",
            "strengths NOUN dobj\n",
            ", PUNCT punct\n",
            "weaknesses NOUN conj\n",
            "and CCONJ cc\n",
            "underlying ADJ amod\n",
            "assumptions NOUN conj\n",
            "of ADP prep\n",
            "NLP PROPN pobj\n",
            "and CCONJ cc\n",
            "text NOUN compound\n",
            "analysis NOUN conj\n",
            "to PART aux\n",
            "apply VERB relcl\n",
            "ethical ADJ amod\n",
            "practices NOUN dobj\n",
            "( PUNCT punct\n",
            "CILO PROPN appos\n",
            "5.1 NUM nummod\n",
            ", PUNCT punct\n",
            "5.2 NUM nummod\n",
            ") PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Learning PROPN dep\n",
            "and CCONJ cc\n",
            "teaching NOUN compound\n",
            "activities NOUN compound\n",
            "\n",
            " SPACE dep\n",
            "Blend PROPN conj\n",
            "of ADP prep\n",
            "online NOUN pobj\n",
            "and CCONJ cc\n",
            "face NOUN conj\n",
            "to PART aux\n",
            "face VERB advcl\n",
            "activities NOUN dobj\n",
            ": PUNCT punct\n",
            "The DET det\n",
            "subject NOUN nsubjpass\n",
            "is AUX auxpass\n",
            "offered VERB ROOT\n",
            "through ADP prep\n",
            "a DET det\n",
            "series NOUN pobj\n",
            "of ADP prep\n",
            "teaching NOUN compound\n",
            "sessions NOUN pobj\n",
            "which PRON nsubj\n",
            "blend VERB relcl\n",
            "online ADV amod\n",
            "and CCONJ cc\n",
            "face NOUN conj\n",
            "- PUNCT punct\n",
            "to ADP prep\n",
            "- PUNCT punct\n",
            "face NOUN pobj\n",
            "learning NOUN dobj\n",
            ". PUNCT punct\n",
            "Students NOUN nsubj\n",
            "learn VERB ROOT\n",
            "through ADP prep\n",
            "interactive ADJ amod\n",
            "lectures NOUN pobj\n",
            "and CCONJ cc\n",
            "classroom NOUN compound\n",
            "activities NOUN conj\n",
            "making VERB acl\n",
            "use NOUN dobj\n",
            "of ADP prep\n",
            "the DET det\n",
            "subject ADJ amod\n",
            "materials NOUN pobj\n",
            "on ADP prep\n",
            "canvas NOUN pobj\n",
            ". PUNCT punct\n",
            "They PRON nsubj\n",
            "also ADV advmod\n",
            "engage VERB ROOT\n",
            "in ADP prep\n",
            "individual ADJ amod\n",
            "and CCONJ cc\n",
            "collaborative ADJ conj\n",
            "learning NOUN conj\n",
            "activities NOUN pobj\n",
            "to ADP prep\n",
            "… PUNCT punct\n",
            "For ADP prep\n",
            "more ADJ amod\n",
            "content NOUN pobj\n",
            "click VERB conj\n",
            "the DET det\n",
            "Read PROPN npadvmod\n",
            "More ADJ amod\n",
            "button NOUN dobj\n",
            "below ADV advmod\n",
            ". PUNCT punct\n",
            "Authentic ADJ amod\n",
            "problem NOUN ROOT\n",
            "based VERB acl\n",
            "learning NOUN pobj\n",
            ": PUNCT punct\n",
            "This DET det\n",
            "subject NOUN nsubj\n",
            "offers VERB ROOT\n",
            "a DET det\n",
            "range NOUN dobj\n",
            "of ADP prep\n",
            "authentic ADJ amod\n",
            "data NOUN compound\n",
            "science NOUN compound\n",
            "problems NOUN pobj\n",
            "to PART aux\n",
            "solve VERB acl\n",
            "that PRON nsubj\n",
            "will AUX aux\n",
            "help VERB ccomp\n",
            "develop VERB xcomp\n",
            "students NOUN poss\n",
            "’ PART case\n",
            "text NOUN compound\n",
            "analysis NOUN compound\n",
            "skills NOUN dobj\n",
            ". PUNCT punct\n",
            "They PRON nsubj\n",
            "work VERB ROOT\n",
            "on ADP prep\n",
            "real ADJ amod\n",
            "world NOUN compound\n",
            "data NOUN compound\n",
            "analysis NOUN compound\n",
            "problems NOUN pobj\n",
            "for ADP prep\n",
            "broad ADJ amod\n",
            "areas NOUN pobj\n",
            "of ADP prep\n",
            "interest NOUN pobj\n",
            "using VERB acl\n",
            "unstructured ADJ amod\n",
            "data NOUN dobj\n",
            "and CCONJ cc\n",
            "contemporary ADJ amod\n",
            "techniques NOUN conj\n",
            ". PUNCT punct\n",
            "Collaborative ADJ amod\n",
            "work NOUN nsubj\n",
            ": PUNCT punct\n",
            "Group NOUN compound\n",
            "activities NOUN nsubj\n",
            "will AUX aux\n",
            "enable VERB ROOT\n",
            "students NOUN dobj\n",
            "to PART aux\n",
            "leverage VERB xcomp\n",
            "peer NOUN compound\n",
            "- PUNCT punct\n",
            "learning NOUN dobj\n",
            "and CCONJ cc\n",
            "demonstrate VERB conj\n",
            "effective ADJ amod\n",
            "team NOUN compound\n",
            "participation NOUN dobj\n",
            ", PUNCT punct\n",
            "as ADV advmod\n",
            "well ADV advmod\n",
            "as ADP cc\n",
            "learning VERB conj\n",
            "to PART aux\n",
            "work VERB xcomp\n",
            "in ADP prep\n",
            "professional ADJ amod\n",
            "teams NOUN pobj\n",
            "with ADP prep\n",
            "an DET det\n",
            "appreciation NOUN pobj\n",
            "of ADP prep\n",
            "diverse ADJ amod\n",
            "perspectives NOUN pobj\n",
            "on ADP prep\n",
            "data NOUN compound\n",
            "science NOUN pobj\n",
            "and CCONJ cc\n",
            "innovation NOUN conj\n",
            ". PUNCT punct\n",
            "Future NOUN npadvmod\n",
            "- PUNCT punct\n",
            "oriented VERB amod\n",
            "strategies NOUN ROOT\n",
            ": PUNCT punct\n",
            "Students NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "exposed VERB acl\n",
            "to ADP prep\n",
            "contemporary ADJ amod\n",
            "learning NOUN compound\n",
            "models NOUN pobj\n",
            "using VERB acl\n",
            "speculative ADJ amod\n",
            "thinking NOUN dobj\n",
            ", PUNCT punct\n",
            "ethical ADJ amod\n",
            "and CCONJ cc\n",
            "human NOUN npadvmod\n",
            "- PUNCT punct\n",
            "centered VERB conj\n",
            "approaches NOUN conj\n",
            "as ADV advmod\n",
            "well ADV advmod\n",
            "as ADP cc\n",
            "reflection NOUN conj\n",
            ". PUNCT punct\n",
            "Electronic ADJ amod\n",
            "portfolios NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "used VERB ROOT\n",
            "to PART aux\n",
            "curate VERB xcomp\n",
            ", PUNCT punct\n",
            "consolidate VERB conj\n",
            "and CCONJ cc\n",
            "provide VERB conj\n",
            "evidence NOUN dobj\n",
            "of ADP prep\n",
            "learning NOUN pobj\n",
            "and CCONJ cc\n",
            "development NOUN conj\n",
            "of ADP prep\n",
            "course NOUN pobj\n",
            "outcomes NOUN conj\n",
            ", PUNCT punct\n",
            "graduate NOUN compound\n",
            "attributes NOUN conj\n",
            "and CCONJ cc\n",
            "professional ADJ amod\n",
            "evolution NOUN conj\n",
            ". PUNCT punct\n",
            "Formative ADJ amod\n",
            "feedback NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "offered VERB ROOT\n",
            "with ADP prep\n",
            "all DET det\n",
            "assessment NOUN compound\n",
            "activities NOUN pobj\n",
            "for ADP prep\n",
            "successful ADJ amod\n",
            "engagement NOUN pobj\n",
            ". PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Authentic ADJ amod\n",
            "problem NOUN ROOT\n",
            "based VERB acl\n",
            "learning NOUN pobj\n",
            ": PUNCT punct\n",
            "This DET det\n",
            "subject NOUN nsubj\n",
            "offers VERB ROOT\n",
            "a DET det\n",
            "range NOUN dobj\n",
            "of ADP prep\n",
            "authentic ADJ amod\n",
            "data NOUN compound\n",
            "science NOUN compound\n",
            "problems NOUN pobj\n",
            "to PART aux\n",
            "solve VERB acl\n",
            "that PRON nsubj\n",
            "will AUX aux\n",
            "help VERB ccomp\n",
            "develop VERB xcomp\n",
            "students NOUN poss\n",
            "’ PART case\n",
            "text NOUN compound\n",
            "analysis NOUN compound\n",
            "skills NOUN dobj\n",
            ". PUNCT punct\n",
            "They PRON nsubj\n",
            "work VERB ROOT\n",
            "on ADP prep\n",
            "real ADJ amod\n",
            "world NOUN compound\n",
            "data NOUN compound\n",
            "analysis NOUN compound\n",
            "problems NOUN pobj\n",
            "for ADP prep\n",
            "broad ADJ amod\n",
            "areas NOUN pobj\n",
            "of ADP prep\n",
            "interest NOUN pobj\n",
            "using VERB acl\n",
            "unstructured ADJ amod\n",
            "data NOUN dobj\n",
            "and CCONJ cc\n",
            "contemporary ADJ amod\n",
            "techniques NOUN conj\n",
            ". PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Collaborative ADJ amod\n",
            "work NOUN nsubj\n",
            ": PUNCT punct\n",
            "Group NOUN compound\n",
            "activities NOUN nsubj\n",
            "will AUX aux\n",
            "enable VERB ROOT\n",
            "students NOUN dobj\n",
            "to PART aux\n",
            "leverage VERB xcomp\n",
            "peer NOUN compound\n",
            "- PUNCT punct\n",
            "learning NOUN dobj\n",
            "and CCONJ cc\n",
            "demonstrate VERB conj\n",
            "effective ADJ amod\n",
            "team NOUN compound\n",
            "participation NOUN dobj\n",
            ", PUNCT punct\n",
            "as ADV advmod\n",
            "well ADV advmod\n",
            "as ADP cc\n",
            "learning VERB conj\n",
            "to PART aux\n",
            "work VERB xcomp\n",
            "in ADP prep\n",
            "professional ADJ amod\n",
            "teams NOUN pobj\n",
            "with ADP prep\n",
            "an DET det\n",
            "appreciation NOUN pobj\n",
            "of ADP prep\n",
            "diverse ADJ amod\n",
            "perspectives NOUN pobj\n",
            "on ADP prep\n",
            "data NOUN compound\n",
            "science NOUN pobj\n",
            "and CCONJ cc\n",
            "innovation NOUN conj\n",
            ". PUNCT punct\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "Future NOUN npadvmod\n",
            "- PUNCT punct\n",
            "oriented VERB amod\n",
            "strategies NOUN ROOT\n",
            ": PUNCT punct\n",
            "Students NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "exposed VERB acl\n",
            "to ADP prep\n",
            "contemporary ADJ amod\n",
            "learning NOUN compound\n",
            "models NOUN pobj\n",
            "using VERB acl\n",
            "speculative ADJ amod\n",
            "thinking NOUN dobj\n",
            ", PUNCT punct\n",
            "ethical ADJ amod\n",
            "and CCONJ cc\n",
            "human NOUN npadvmod\n",
            "- PUNCT punct\n",
            "centered VERB conj\n",
            "approaches NOUN conj\n",
            "as ADV advmod\n",
            "well ADV advmod\n",
            "as ADP cc\n",
            "reflection NOUN conj\n",
            ". PUNCT punct\n",
            "Electronic ADJ amod\n",
            "portfolios NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "used VERB ROOT\n",
            "to PART aux\n",
            "curate VERB xcomp\n",
            ", PUNCT punct\n",
            "consolidate VERB conj\n",
            "and CCONJ cc\n",
            "provide VERB conj\n",
            "evidence NOUN dobj\n",
            "of ADP prep\n",
            "learning NOUN pobj\n",
            "and CCONJ cc\n",
            "development NOUN conj\n",
            "of ADP prep\n",
            "course NOUN pobj\n",
            "outcomes NOUN conj\n",
            ", PUNCT punct\n",
            "graduate NOUN compound\n",
            "attributes NOUN conj\n",
            "and CCONJ cc\n",
            "professional ADJ amod\n",
            "evolution NOUN conj\n",
            ". PUNCT punct\n",
            "Formative ADJ amod\n",
            "feedback NOUN nsubjpass\n",
            "will AUX aux\n",
            "be AUX auxpass\n",
            "offered VERB ROOT\n",
            "with ADP prep\n",
            "all DET det\n",
            "assessment NOUN compound\n",
            "activities NOUN pobj\n",
            "for ADP prep\n",
            "successful ADJ amod\n",
            "engagement NOUN pobj\n",
            ". PUNCT punct\n",
            "\n",
            "\n",
            "\n",
            " SPACE dep\n",
            "\n",
            "Sentences:\n",
            "\n",
            "36118 Applied Natural Language Processing\n",
            "\n",
            "Subject description\n",
            "This subject introduces you to the complexities of analysing human language data and the use of Natural Language Processing (NLP) and text mining techniques.\n",
            "You'll develop both technical and communicative skills to process and interpret unstructured textual data.\n",
            "Covering core NLP concepts and extraction techniques, the course equips you with … For more content click the Read More button below.\n",
            "\n",
            "\n",
            "Learning outcomes\n",
            "Upon completion of this subject, graduates will be able to:\n",
            "\n",
            "1. Understand core concepts of Natural Language Processing (NLP) and computational linguistics including its limitations (CILO 2.2, 2.3)\n",
            "\n",
            "2.Evaluate complex challenges for problem solving and build practical NLP applications (CILO 2.3, 4.2)\n",
            "\n",
            "3.\n",
            "Apply text mining techniques on unstructured data sets using advanced NLP programming packages (CILOs 1.2, 2.2)\n",
            "\n",
            "4.\n",
            "Interpret, extract value and effectively communicate insights from text analysis and create real-world applications suitable to a range of audiences (CILOs 2.4, 3.2, 4.2)\n",
            "\n",
            "5.\n",
            "Articulate the strengths, weaknesses and underlying assumptions of NLP and text analysis to apply ethical practices (CILO 5.1, 5.2)\n",
            "\n",
            "Learning and teaching activities\n",
            "Blend of online and face to face activities: The subject is offered through a series of teaching sessions which blend online and face-to-face learning.\n",
            "Students learn through interactive lectures and classroom activities making use of the subject materials on canvas.\n",
            "They also engage in individual and collaborative learning activities to … For more content click the Read More button below.\n",
            "Authentic problem based learning:\n",
            "This subject offers a range of authentic data science problems to solve that will help develop students’ text analysis skills.\n",
            "They work on real world data analysis problems for broad areas of interest using unstructured data and contemporary techniques.\n",
            "Collaborative work: Group activities will enable students to leverage peer-learning and demonstrate effective team participation, as well as learning to work in professional teams with an appreciation of diverse perspectives on data science and innovation.\n",
            "Future-oriented strategies: Students will be exposed to contemporary learning models using speculative thinking, ethical and human-centered approaches as well as reflection.\n",
            "Electronic portfolios will be used to curate, consolidate and provide evidence of learning and development of course outcomes, graduate attributes and professional evolution.\n",
            "Formative feedback will be offered with all assessment activities for successful engagement.\n",
            "\n",
            "\n",
            "Authentic problem based learning:\n",
            "This subject offers a range of authentic data science problems to solve that will help develop students’ text analysis skills.\n",
            "They work on real world data analysis problems for broad areas of interest using unstructured data and contemporary techniques.\n",
            "\n",
            "\n",
            "Collaborative work: Group activities will enable students to leverage peer-learning and demonstrate effective team participation, as well as learning to work in professional teams with an appreciation of diverse perspectives on data science and innovation.\n",
            "\n",
            "\n",
            "Future-oriented strategies: Students will be exposed to contemporary learning models using speculative thinking, ethical and human-centered approaches as well as reflection.\n",
            "Electronic portfolios will be used to curate, consolidate and provide evidence of learning and development of course outcomes, graduate attributes and professional evolution.\n",
            "Formative feedback will be offered with all assessment activities for successful engagement.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Named Entities:\n",
            "36118 DATE\n",
            "Applied Natural Language Processing\n",
            "\n",
            "Subject ORG\n",
            "Natural Language Processing (NLP WORK_OF_ART\n",
            "NLP ORG\n",
            "1 CARDINAL\n",
            "Natural Language Processing ORG\n",
            "NLP ORG\n",
            "2.2 CARDINAL\n",
            "2.3 CARDINAL\n",
            "NLP ORG\n",
            "2.3 CARDINAL\n",
            "4.2 CARDINAL\n",
            "3 CARDINAL\n",
            "NLP ORG\n",
            "1.2 CARDINAL\n",
            "2.2 CARDINAL\n",
            "4 CARDINAL\n",
            "2.4 CARDINAL\n",
            "3.2 DATE\n",
            "4.2 CARDINAL\n",
            "5 CARDINAL\n",
            "NLP ORG\n",
            "5.1 CARDINAL\n",
            "5.2 CARDINAL\n",
            "Authentic NORP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokenizer\n",
        "Create a custom tokenizer in Python that handles:\n",
        "*   Contractions (e.g., \"don't\" → \"do n't\")\n",
        "*   Keeps punctuation as separate tokens\n",
        "*   Splits hyphenated words (e.g., \"state-of-the-art\" → \"state of the art\")\n",
        "\n",
        "Compare its results with NLTK's word_tokenize on any sample paragraph and the following examples:\n",
        "\"New York-based company\", \"It's a beautiful day!\", \"https://www.example.com\"\n",
        "\n",
        "What differences do you see? What are the advantages, and limitations of each approach?"
      ],
      "metadata": {
        "id": "fER49EQBRZlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk # Added import statement for nltk\n",
        "nltk.download('punkt_tab') # Download the necessary data package\n",
        "\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    # Handle contractions\n",
        "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1 '\\2\", text)  # \"don't\" -> \"do n't\"\n",
        "    text = re.sub(r\"n't\", \" n't\", text)\n",
        "\n",
        "    # Keep punctuation\n",
        "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)  # Add space around punctuation\n",
        "\n",
        "    # Split hyphenated words\n",
        "    text = re.sub(r\"([a-zA-Z])-([a-zA-Z])\", r\"\\1 \\2\", text)\n",
        "\n",
        "    # Handle URLs\n",
        "    text = re.sub(r\"(https?://\\S+)\", r\" \\1 \", text)\n",
        "\n",
        "    # Handle other special cases\n",
        "    tokens = text.split()\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Sample paragraph\n",
        "paragraph = \"It's a beautiful day! New York-based company announced state-of-the-art technology. Let's go to https://www.example.com.\"\n",
        "\n",
        "# Tokenization using custom tokenizer and NLTK\n",
        "custom_tokens = custom_tokenize(user_input)\n",
        "nltk_tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Example sentences\n",
        "examples = [\"New York-based company\", \"It's a beautiful day!\", \"https://www.example.com\"]\n",
        "\n",
        "print(\"Custom Tokenizer:\")\n",
        "print(custom_tokens)\n",
        "print(\"\\nNLTK Tokenizer:\")\n",
        "print(nltk_tokens)\n",
        "print(\"\\n\")\n",
        "\n",
        "for example in examples:\n",
        "    print(\"Example:\", example)\n",
        "    print(\"Custom Tokenizer:\", custom_tokenize(example))\n",
        "    print(\"NLTK Tokenizer:\", word_tokenize(example))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "ZY_8KMjNz6WO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d315d85-b936-49e9-b210-474488d8b1a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokenizer:\n",
            "['36118', 'Applied', 'Natural', 'Language', 'Processing', 'Subject', 'description', 'This', 'subject', 'introduces', 'you', 'to', 'the', 'complexities', 'of', 'analysing', 'human', 'language', 'data', 'and', 'the', 'use', 'of', 'Natural', 'Language', 'Processing', '(NLP)', 'and', 'text', 'mining', 'techniques', '.', 'You', \"'ll\", 'develop', 'both', 'technical', 'and', 'communicative', 'skills', 'to', 'process', 'and', 'interpret', 'unstructured', 'textual', 'data', '.', 'Covering', 'core', 'NLP', 'concepts', 'and', 'extraction', 'techniques', ',', 'the', 'course', 'equips', 'you', 'with', '…', 'For', 'more', 'content', 'click', 'the', 'Read', 'More', 'button', 'below', '.', 'Learning', 'outcomes', 'Upon', 'completion', 'of', 'this', 'subject', ',', 'graduates', 'will', 'be', 'able', 'to', ':', '1', '.', 'Understand', 'core', 'concepts', 'of', 'Natural', 'Language', 'Processing', '(NLP)', 'and', 'computational', 'linguistics', 'including', 'its', 'limitations', '(CILO', '2', '.', '2', ',', '2', '.', '3)', '2', '.', 'Evaluate', 'complex', 'challenges', 'for', 'problem', 'solving', 'and', 'build', 'practical', 'NLP', 'applications', '(CILO', '2', '.', '3', ',', '4', '.', '2)', '3', '.', 'Apply', 'text', 'mining', 'techniques', 'on', 'unstructured', 'data', 'sets', 'using', 'advanced', 'NLP', 'programming', 'packages', '(CILOs', '1', '.', '2', ',', '2', '.', '2)', '4', '.', 'Interpret', ',', 'extract', 'value', 'and', 'effectively', 'communicate', 'insights', 'from', 'text', 'analysis', 'and', 'create', 'real', 'world', 'applications', 'suitable', 'to', 'a', 'range', 'of', 'audiences', '(CILOs', '2', '.', '4', ',', '3', '.', '2', ',', '4', '.', '2)', '5', '.', 'Articulate', 'the', 'strengths', ',', 'weaknesses', 'and', 'underlying', 'assumptions', 'of', 'NLP', 'and', 'text', 'analysis', 'to', 'apply', 'ethical', 'practices', '(CILO', '5', '.', '1', ',', '5', '.', '2)', 'Learning', 'and', 'teaching', 'activities', 'Blend', 'of', 'online', 'and', 'face', 'to', 'face', 'activities', ':', 'The', 'subject', 'is', 'offered', 'through', 'a', 'series', 'of', 'teaching', 'sessions', 'which', 'blend', 'online', 'and', 'face', 'to', 'face', 'learning', '.', 'Students', 'learn', 'through', 'interactive', 'lectures', 'and', 'classroom', 'activities', 'making', 'use', 'of', 'the', 'subject', 'materials', 'on', 'canvas', '.', 'They', 'also', 'engage', 'in', 'individual', 'and', 'collaborative', 'learning', 'activities', 'to', '…', 'For', 'more', 'content', 'click', 'the', 'Read', 'More', 'button', 'below', '.', 'Authentic', 'problem', 'based', 'learning', ':', 'This', 'subject', 'offers', 'a', 'range', 'of', 'authentic', 'data', 'science', 'problems', 'to', 'solve', 'that', 'will', 'help', 'develop', 'students’', 'text', 'analysis', 'skills', '.', 'They', 'work', 'on', 'real', 'world', 'data', 'analysis', 'problems', 'for', 'broad', 'areas', 'of', 'interest', 'using', 'unstructured', 'data', 'and', 'contemporary', 'techniques', '.', 'Collaborative', 'work', ':', 'Group', 'activities', 'will', 'enable', 'students', 'to', 'leverage', 'peer', 'learning', 'and', 'demonstrate', 'effective', 'team', 'participation', ',', 'as', 'well', 'as', 'learning', 'to', 'work', 'in', 'professional', 'teams', 'with', 'an', 'appreciation', 'of', 'diverse', 'perspectives', 'on', 'data', 'science', 'and', 'innovation', '.', 'Future', 'oriented', 'strategies', ':', 'Students', 'will', 'be', 'exposed', 'to', 'contemporary', 'learning', 'models', 'using', 'speculative', 'thinking', ',', 'ethical', 'and', 'human', 'centered', 'approaches', 'as', 'well', 'as', 'reflection', '.', 'Electronic', 'portfolios', 'will', 'be', 'used', 'to', 'curate', ',', 'consolidate', 'and', 'provide', 'evidence', 'of', 'learning', 'and', 'development', 'of', 'course', 'outcomes', ',', 'graduate', 'attributes', 'and', 'professional', 'evolution', '.', 'Formative', 'feedback', 'will', 'be', 'offered', 'with', 'all', 'assessment', 'activities', 'for', 'successful', 'engagement', '.', 'Authentic', 'problem', 'based', 'learning', ':', 'This', 'subject', 'offers', 'a', 'range', 'of', 'authentic', 'data', 'science', 'problems', 'to', 'solve', 'that', 'will', 'help', 'develop', 'students’', 'text', 'analysis', 'skills', '.', 'They', 'work', 'on', 'real', 'world', 'data', 'analysis', 'problems', 'for', 'broad', 'areas', 'of', 'interest', 'using', 'unstructured', 'data', 'and', 'contemporary', 'techniques', '.', 'Collaborative', 'work', ':', 'Group', 'activities', 'will', 'enable', 'students', 'to', 'leverage', 'peer', 'learning', 'and', 'demonstrate', 'effective', 'team', 'participation', ',', 'as', 'well', 'as', 'learning', 'to', 'work', 'in', 'professional', 'teams', 'with', 'an', 'appreciation', 'of', 'diverse', 'perspectives', 'on', 'data', 'science', 'and', 'innovation', '.', 'Future', 'oriented', 'strategies', ':', 'Students', 'will', 'be', 'exposed', 'to', 'contemporary', 'learning', 'models', 'using', 'speculative', 'thinking', ',', 'ethical', 'and', 'human', 'centered', 'approaches', 'as', 'well', 'as', 'reflection', '.', 'Electronic', 'portfolios', 'will', 'be', 'used', 'to', 'curate', ',', 'consolidate', 'and', 'provide', 'evidence', 'of', 'learning', 'and', 'development', 'of', 'course', 'outcomes', ',', 'graduate', 'attributes', 'and', 'professional', 'evolution', '.', 'Formative', 'feedback', 'will', 'be', 'offered', 'with', 'all', 'assessment', 'activities', 'for', 'successful', 'engagement', '.']\n",
            "\n",
            "NLTK Tokenizer:\n",
            "['It', \"'s\", 'a', 'beautiful', 'day', '!', 'New', 'York-based', 'company', 'announced', 'state-of-the-art', 'technology', '.', 'Let', \"'s\", 'go', 'to', 'https', ':', '//www.example.com', '.']\n",
            "\n",
            "\n",
            "Example: New York-based company\n",
            "Custom Tokenizer: ['New', 'York', 'based', 'company']\n",
            "NLTK Tokenizer: ['New', 'York-based', 'company']\n",
            "\n",
            "\n",
            "Example: It's a beautiful day!\n",
            "Custom Tokenizer: ['It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "NLTK Tokenizer: ['It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "\n",
            "\n",
            "Example: https://www.example.com\n",
            "Custom Tokenizer: ['https', ':', '//www', '.', 'example', '.', 'com']\n",
            "NLTK Tokenizer: ['https', ':', '//www.example.com']\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Regex\n",
        "\n",
        "Try writing your own RegEx that can capture citations in text E.g. (Horning, 2022)"
      ],
      "metadata": {
        "id": "iQSBeMtujknV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_citations(text):\n",
        "    # Regular expression to match citations in the format (Author, Year)\n",
        "    citation_pattern = r\"\\(([A-Z][a-z]+(?:[\\s-][A-Z][a-z]+)?),\\s(\\d{4})\\)\"\n",
        "    citations = re.findall(citation_pattern, text)\n",
        "    return citations\n",
        "\n",
        "# Example usage (using the user_input from the previous code block)\n",
        "citations = extract_citations(user_input)\n",
        "print(\"Citations found:\")\n",
        "for citation in citations:\n",
        "    print(citation)\n",
        "\n",
        "\n",
        "# Example with additional test cases\n",
        "test_cases = [\n",
        "    \"This is a sentence with a citation (Horning, 2022).\",\n",
        "    \"Another citation (Smith-Jones, 2023) is here.\",\n",
        "    \"No citations here.\",\n",
        "    \"(Doe, 1999) and (Jane Doe, 2000).\"\n",
        "]\n",
        "\n",
        "for text in test_cases:\n",
        "    citations = extract_citations(text)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Citations: {citations}\")\n"
      ],
      "metadata": {
        "id": "iI5xT7o8Rlcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4d992f-0c78-4f61-f7ac-5c11c4e69c3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Citations found:\n",
            "\n",
            "Text: This is a sentence with a citation (Horning, 2022).\n",
            "Citations: [('Horning', '2022')]\n",
            "\n",
            "Text: Another citation (Smith-Jones, 2023) is here.\n",
            "Citations: [('Smith-Jones', '2023')]\n",
            "\n",
            "Text: No citations here.\n",
            "Citations: []\n",
            "\n",
            "Text: (Doe, 1999) and (Jane Doe, 2000).\n",
            "Citations: [('Doe', '1999'), ('Jane Doe', '2000')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract URLS following a certain format (www. or http or https:// ..)"
      ],
      "metadata": {
        "id": "DCuiR4kZCp32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Extract URLS following a certain format (www. or http or https:// ..)\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_urls(text):\n",
        "    url_pattern = r\"(https?://\\S+|www\\.\\S+)\"\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    return urls\n",
        "\n",
        "# Example usage (using the user_input from the previous code block)\n",
        "urls = extract_urls(user_input)\n",
        "print(\"URLs found:\")\n",
        "for url in urls:\n",
        "  url\n"
      ],
      "metadata": {
        "id": "V49uczwjCyd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292d9f28-3047-4291-dbfe-cecf93eda0a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLs found:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Word Frequency\n",
        "\n",
        "Find the list of words that occur more than 10 times in a selected corpus.\n",
        "\n",
        "Try using different forms of setup: no stopwords, custom stopwords, not removing punctuation, etc. and see what difference in results they produce.\n"
      ],
      "metadata": {
        "id": "tBIAVwtV_cvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def word_frequency(text, min_frequency=10, remove_stopwords=True, custom_stopwords=None, remove_punctuation=True):\n",
        "\n",
        "    # 1. Tokenization (similar to the custom_tokenize function)\n",
        "    text = re.sub(r\"(\\w)'(\\w)\", r\"\\1 '\\2\", text)\n",
        "    text = re.sub(r\"n't\", \" n't\", text)\n",
        "    if remove_punctuation:\n",
        "        text = re.sub(r\"[^\\w\\s']\", \"\", text) #remove punctuation\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # 2. Stopword Removal\n",
        "    if remove_stopwords:\n",
        "        stopwords = set(['the', 'a', 'an', 'and', 'or', 'in', 'to', 'of', 'for', 'with', 'on', 'at', 'by', 'from', 'up', 'down', 'out', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'should', 'can', 'could', 'may', 'might', 'must', 'as', 'if', 'so', 'that', 'this', 'these', 'those', 'it', 'its', 'they', 'their', 'them', 'he', 'him', 'his', 'she', 'her', 'hers', 'we', 'us', 'our', 'ours', 'you', 'your', 'yours', 'my', 'mine', 'i', 'me'])\n",
        "        if custom_stopwords:\n",
        "            stopwords.update(custom_stopwords)  # Add custom stopwords if provided\n",
        "        tokens = [token for token in tokens if token not in stopwords]\n",
        "\n",
        "    # 3. Count word frequencies\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # 4. Return words with frequency > min_frequency\n",
        "    frequent_words = [(word, count) for word, count in word_counts.items() if count > min_frequency]\n",
        "    return frequent_words\n",
        "\n",
        "# Example usage with the provided user_input\n",
        "user_input = '''\n",
        "In a selected corpus, we can find the list of words that occur more than 10 times.\n",
        "For example, in this dummy text, we will try to use different forms of setup: no stopwords, custom stopwords, not removing punctuation, etc., and see what difference in results they produce.\n",
        " The word ‘corpus’ appears multiple times in this text. The word ‘text’ also appears multiple times. We will also include some common stopwords like ‘the’, ‘and’, ‘in’, ‘of’, etc., to see how they affect the results.\n",
        " Additionally, we will add some punctuation marks like commas, periods, and exclamation marks! Let’s see how this dummy text helps in understanding the concept of corpus and word frequency analysis. corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus corpus text text text text text text text text text text text text text text text stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords stopwords punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation punctuation analysis analysis analysis analysis analysis analysis analysis analysis analysis analysis analysis analysis\n",
        "'''\n",
        "\n",
        "\n",
        "# Different setups\n",
        "print(\"No Stopwords, No Punctuation Removal:\")\n",
        "print(word_frequency(user_input, min_frequency=2, remove_stopwords=False, remove_punctuation=False))\n",
        "\n",
        "\n",
        "print(\"\\nDefault Settings (remove stopwords, remove punctuation):\")\n",
        "print(word_frequency(user_input, min_frequency=2))\n",
        "\n",
        "\n",
        "print(\"\\nWith Custom Stopwords and Punctuation:\")\n",
        "custom_stops = {'subject', 'learning'}\n",
        "print(word_frequency(user_input, min_frequency=2, custom_stopwords=custom_stops, remove_punctuation=True))\n"
      ],
      "metadata": {
        "id": "tNK4qAHUFUxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81efbeae-8acf-4439-ffe1-0f7647ce5b52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Stopwords, No Punctuation Removal:\n",
            "[('in', 5), ('we', 4), ('the', 5), ('of', 3), ('this', 3), ('will', 3), ('and', 3), ('see', 3), ('word', 3), ('stopwords', 15), ('punctuation', 16), ('text', 16), ('corpus', 12), ('analysis', 12)]\n",
            "\n",
            "Default Settings (remove stopwords, remove punctuation):\n",
            "[('corpus', 14), ('times', 3), ('text', 19), ('stopwords', 17), ('punctuation', 17), ('see', 3), ('word', 3), ('analysis', 13)]\n",
            "\n",
            "With Custom Stopwords and Punctuation:\n",
            "[('corpus', 14), ('times', 3), ('text', 19), ('stopwords', 17), ('punctuation', 17), ('see', 3), ('word', 3), ('analysis', 13)]\n"
          ]
        }
      ]
    }
  ]
}